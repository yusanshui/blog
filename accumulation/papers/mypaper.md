### 应用于大数据可视分析平台的云原生数据存储系统
A Cloud-based data storage system for big data visualization platform

#### abstract

面对数量庞大的数据和不断扩展的可视化系统，建立一套高效的存储支持nebula平台十分有意义。大数据可视系统需要对海量数据进行存储，并保证数据的不丢失，服务类型也会随着平台功能的扩展而增加。因此，平台需要一套能够支持容量横向扩展、存储可靠的存储系统。本文面对数据可视化系统中面对的存储挑战设计了一套分布式存储系统，该系统具有高可用、多副本、容灾、共享存储等特点，最重要的是满足存储卷动态供给、应用分配的存储可调整、集群存储容量支持横向扩展等存储需求。本文还涉及了一系列辅助系统和监控系统，帮助用户快速调整集群配置、监控集群节点状态和应用日志。


#### introduction （introduction to the nebula project)

大数据时代，运用好数据可以给社会或者企业带来巨大的效益。数据可视化，是为了优化数据表达，更直观地让我们了解数据的情况。目前，对于小数据我们可以通过一些数据分析软件如excel，spss或者编程语言进行数据分析，但面对大数据，这些软件就显得捉襟见肘，人们只能通过编程语言来进行数据分析，这就需要具有专业的技能。总而言之，如果想要对大数据做分析，还要专门去学一门编程语言，无疑增加了大数据分析的门槛。为此，我们搭建了一套数据可视分析平台，可以减少我们进行大数据分析的难度。

平台架构为：


本文重点关注平台存储系统的设计和实现。

#### requirements and challenges

平台对存储系统的要求有：

1. 动态供给：随着数据分析平台功能的扩展，服务类型也会不断增加，需要存储能够动态划分存储卷提供给新的服务使用
2. 共享存储：支持共享存储，保证数据一致性
3. 容量变更：在应用层面，可视分析平台台是处理大数据，数据集数量会随着时间不断扩展，分配给应用的存储卷例如数据库的存储会随着数据量的增加需要不断扩展；同时，整个集群随着系统功能的增加，对存储的需要也会不断增加，因此整个集群的存储也需要可以扩展。
4. 设备容灾：磁盘损坏或节点损坏，不会造成数据的丢失
5. 多副本：通过多副本，实现高可用和缓解热点读
6. 磁盘、设备可更换：磁盘更换后可以重新注册为存储单元，将原数据复制到新磁盘，新磁盘代替原从磁盘提供存储服务，服务器设备更换，新服务器加入集群中，可以将原服务器中的数据复制到新服务器，原服务器废弃，新服务器代替原服务器提供存储服务


#### related work（existing technology)

#### system arch
平台架构如图所示：

第一层：应用层
对于应用来说，只需要指定申请的存储卷类型和大小，就可以得到一份存储卷。查看应用的状态可以看到分配给实例的存储卷，从而可以使多个应用挂载到同一个存储卷上，实现存储共享，应用可以动态的调整申请存储卷的大小。

第二层：动态供给存储单元层
在这一层，将分布式存储系统层提供的各种存储系统格式形成不同类型的存储池，当应用层申请某种指定类型的存储时，动态供给层会从相应的类型中划分出指定大小的存储分配给实例。


第三层：分布式存储系统层
分布式存储层最重要的实现多副本，保证高可用和缓解热点读。分布式存储需要将数据的不同副本放在不同的节点上或磁盘上，保证节点或设备损坏时，不会造成数据的丢失。

第四层：抽象存储单元层
抽象存储单元层将底层不同的物理设备映射成统一格式标记的存储单元，便于分布式系统部署时将不同数据副本调度到不同的结点或设备上。

第五层：物理机层
整个存储系统的存储容量受物理层存储容量的限制，物理机层需要能够识别不同类型的存储磁盘，并保证新的服务器能够加入到节点上。


#### implemention
kubernetes是一个优秀的集群管理工具。
我们可以利用kubernetes中的local provisioner实现物理机到抽象存储单元的映射。
ceph是一个优秀的分布式存储系统。它支持可扩展，多副本并提供了很好的I/O性能。
rook ceph提供了一种在ceph在k8s集群上的实现。

#### 性能对比

#### 功能模块
为了方便用户对存储实例的调整，我们实现了用户调整实例存储单元的接口，减少用户修改复杂配置文件的工作。

为了方便用户对集群的调整，我们实现了实现了控制磁盘设备加入存储系统的接口，便于用户调整磁盘设备。

#### 日志系统和监控系统

#### feature work

写平台的其他功能








